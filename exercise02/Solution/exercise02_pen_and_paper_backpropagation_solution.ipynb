{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pen & Paper: Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a forward and backward pass to calculate the gradients for the weights $w_0, w_1, w_2, w_s$ in the following MLP. Each node represents one unit with a weight $w_i, i \\in \\{0, 1, 2\\}$ connecting it to the previous node. The connection from unit 0 to unit 2 is called a _skip connection_, which means unit 2 receives input from two sources and thus has an additional weight $w_s$. The weighted inputs are added before the nonlinearity is applied.\n",
    "\n",
    "**![There should be an image here. If you can't see it, you probably forgot to download the mlp.png!](mlp.png)**\n",
    "\n",
    "We assume that we want to solve a regression task. We use an L1-loss $L(\\hat{y}, y) = |y - \\hat{y}|$\n",
    "\n",
    "The nonlinearities for the first two units are rectified linear functions/units (ReLU): $g_0(z) = g_1(z) = \\begin{cases} 0, z<0\\\\ z, else \\end{cases}$.\n",
    "\n",
    "We do not use a nonlinearity for the second unit: $g_2(z_2) = z_2$.\n",
    "\n",
    "**Note:** We use the notation of the Deep Learning book here, i.e. $z = Wx+b$. If you attended the Machine Learning course, you might be used to the different notation used in the Bishop Book, where $z$ denotes the value after applying the activation function. Here, $z$ is the value before applying the activation function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Complete the backpropagation algorithm**\n",
    "\n",
    "Reuse equations/values when possible.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\textbf{forward pass} &&\n",
    "    \\textbf{backward pass} \\\\\n",
    "    L(y, \\hat{y}) = |y-\\hat{y}| &&\n",
    "     \\\\\n",
    "    \\hat{y}=g_2(z_2)=z_2 &&\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} = \\begin{cases} 1, \\hat{y} > y \\\\ -1, else \\end{cases} \\\\\n",
    "    z_2 = w_2 \\cdot h_1 + w_s \\cdot h_0 &&\n",
    "    \\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot 1 \\\\\n",
    "    h_1 = g_1(z_1) = \\begin{cases} 0, z_1<0 \\\\ z_1, else \\end{cases} && \n",
    "    \\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_1} = \\frac{\\partial L}{\\partial z_2} \\cdot w_2 \\\\\n",
    "    z_1 = w_1 \\cdot h_0 &&\n",
    "    \\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial h_1} \\cdot \\frac{\\partial h_1}{\\partial z_1} = \\frac{\\partial L}{\\partial h_1} \\cdot \\begin{cases} 0, z_1 < 0 \\\\ 1, else \\end{cases} \\\\\n",
    "    h_0 = g_0(z_0) = \\begin{cases} 0, z_0<0 \\\\ z_0, else \\end{cases} && \n",
    "    \\frac{\\partial L}{\\partial h_0} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial h_0} + \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial h_0} = \\frac{\\partial L}{\\partial z_2} \\cdot w_s + \\frac{\\partial L}{\\partial z_1} \\cdot w_1 \\\\\n",
    "    z_0 = w_0 \\cdot x &&\n",
    "    \\frac{\\partial L}{\\partial z_0} = \\frac{\\partial L}{\\partial h_0} \\cdot \\frac{\\partial h_0}{\\partial z_0} = \\frac{\\partial L}{\\partial h_0} \\cdot \\begin{cases} 0, z_0 < 0 \\\\ 1, else \\end{cases} \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2} = \\frac{\\partial L}{\\partial z_2} \\cdot h_1 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_s} = \\frac{\\partial L}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_s} = \\frac{\\partial L}{\\partial z_2} \\cdot h_0 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1} = \\frac{\\partial L}{\\partial z_1} \\cdot h_0 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_0} = \\frac{\\partial L}{\\partial z_0} \\cdot \\frac{\\partial z_0}{\\partial w_0} = \\frac{\\partial L}{\\partial z_0} \\cdot x \\\\\n",
    "\\end{align*}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What difference does the skip connection make when propagating back the error?** \n",
    "\n",
    "** Answer: ** Regarding the backpropagation, the third layer receives input from two sources, therefore the gradient flows as well from the third to the first layer in the backward pass. The result is an additional term in the corresponding equations.\n",
    "\n",
    "Regarding the effectiveness of training, this helps to reduce the (vanishing gradient problem)[https://en.wikipedia.org/wiki/Vanishing_gradient_problem] . If we have many layers in our network, the magnitude of gradients can shrink towards the first layers to a point where they basically are not trained. Skip connections help bypass this by propagating the gradient uninterrupted from the last layers. With increasingly deep networks, the are applied quite often in modern architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the gradients for the given datapoint and the given initial weights (calculating the gradients requires to calculate a forward pass first). Also calculate the weights and the loss after one gradient descent step.**\n",
    "\n",
    "$$(x_1, y_1) = (1, -3) \\\\\n",
    "w_0 = w_1 = w_2 = w_s = 0.5 \\\\\n",
    "Learning Rate = 1 \\\\  $$\n",
    "\n",
    "\n",
    "**Complete the backpropagation algorithm**\n",
    "\n",
    "Reuse equations/values when possible.\n",
    "\n",
    "\\begin{align*}\n",
    "    \\textbf{forward pass} &&\n",
    "    \\textbf{backward pass} \\\\\n",
    "    L(y, \\hat{y}) = |y-\\hat{y}| = 3.325 &&\n",
    "     \\\\\n",
    "    \\hat{y}=g_2(z_2)=z_2 = 0.325 &&\n",
    "    \\frac{\\partial L}{\\partial \\hat{y}} = \\begin{cases} 1, \\hat{y} > y \\\\ -1, else \\end{cases} = 1 \\\\\n",
    "    z_2 = w_2 \\cdot h_1 + w_s \\cdot h_0 = 0.325 &&\n",
    "    \\frac{\\partial L}{\\partial z_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot 1 = 1 \\\\\n",
    "    h_1 = \\begin{cases} 0, z_1<0 \\\\ z_1, else \\end{cases} = 0.25 && \n",
    "    \\frac{\\partial L}{\\partial h_1} = \\frac{\\partial L}{\\partial z_2} \\cdot w_2 = 0.5 \\\\\n",
    "    z_1 = w_1 \\cdot h_0 = 0.25 &&\n",
    "    \\frac{\\partial L}{\\partial z_1} = \\frac{\\partial L}{\\partial h_1} \\cdot \\begin{cases} 0, z_1 < 0 \\\\ 1, else \\end{cases} = 0.5 \\\\\n",
    "    h_0 = \\begin{cases} 0, z_0<0 \\\\ z_0, else \\end{cases} = 0.5 && \n",
    "    \\frac{\\partial L}{\\partial h_0} = \\frac{\\partial L}{\\partial z_2} \\cdot w_s + \\frac{\\partial L}{\\partial z_1} \\cdot w_1 = 0.75 \\\\\n",
    "    z_0 = w_0 \\cdot x = 0.5 &&\n",
    "    \\frac{\\partial L}{\\partial z_0} = \\frac{\\partial L}{\\partial h_0} \\cdot \\begin{cases} 0, z_0 < 0 \\\\ 1, else \\end{cases} = 0.75 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_2} = \\frac{\\partial L}{\\partial z_2} \\cdot h_1 = 0.25 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_s} = \\frac{\\partial L}{\\partial z_2} \\cdot h_0 = 0.5 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial z_1} \\cdot h_0 = 0.25 \\\\\n",
    "     &&\n",
    "    \\frac{\\partial L}{\\partial w_0} = \\frac{\\partial L}{\\partial z_0} \\cdot x = 0.75 \\\\\n",
    "\\end{align*}   \n",
    "\n",
    "$\n",
    "\\textbf{gradient descent step} \\\\\n",
    "w_2 = w_2 - 1 \\cdot 0.25 = 0.25 \\\\\n",
    "w_s = w_s - 1 \\cdot 0.5 = 0 \\\\\n",
    "w_1 = w_1 - 1 \\cdot 0.25 = 0.25 \\\\\n",
    "w_0 = w_0 - 1 \\cdot 0.75 = -0.25 \\\\\n",
    "$\n",
    "\n",
    "\\begin{align*}\n",
    "    \\textbf{forward pass} \\\\\n",
    "    L(y, \\hat{y}) = |y-\\hat{y}| = 3 \\\\\n",
    "    \\hat{y}=g_2(z_2)=z_2 = 0 \\\\\n",
    "    z_2 = w_2 \\cdot h_1 + w_s \\cdot h_0 = 0 \\\\\n",
    "    h_1 = \\begin{cases} 0, z_1<0 \\\\ z_1, else \\end{cases} = 0 \\\\\n",
    "    z_1 = w_1 \\cdot h_0 = 0 \\\\\n",
    "    h_0 = \\begin{cases} 0, z_0<0 \\\\ z_0, else \\end{cases} = 0 \\\\\n",
    "    z_0 = w_0 \\cdot x = -0.25 \\\\\n",
    "\\end{align*}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Your feedback on exercise 2.1: ** \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
