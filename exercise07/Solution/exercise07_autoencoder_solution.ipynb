{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seventh Exercise (Chapter 3 & Chapter 14)\n",
    "\n",
    "In the first part we will recap basics of probability theory. \n",
    "\n",
    "In the second part, we're going to implement an autoencoder and explore the learned latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Probability\n",
    "\n",
    "Solve the first two exercises mathematically. Answer using markdown and latex in this notebook. Please provide intermediate steps and explanations. \n",
    "\n",
    "### 7.1.1 Bertrand's Box Paradox\n",
    "\n",
    "Consider a box containing the following 3 cards\n",
    "\n",
    "- one with two black sides\n",
    "- one with two white sides\n",
    "- one with a black and a white side\n",
    "\n",
    "From the three cards, one is drawn at random and put on the table. You can only see the side facing up.\n",
    "\n",
    "**Questions:** \n",
    "\n",
    "a) What are the probabilities that the card on the table shows a black side? What are the probabilities it shows a white side?   \n",
    "b) If we draw a card and it shows black, compute the probability that the other side of the card is also black.  \n",
    "c) Find the the probability that the other side of the card is black if the card shows a white side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:** **TODO**\n",
    "\n",
    "Before we compute any probability, let's define the random variables.\n",
    "We enumerate the cards and use $C \\in \\{1,2,3\\}$ as the corresponding random variables.\n",
    "For the side facing up, we define $ U \\in \\{ b, w\\}$, and similarly for the one facing down $ D \\in \\{b,w\\}$\n",
    "\n",
    "- a) To compute the marginal probability for the card showing a black side, we rewrite it in terms of conditional probabilities:\n",
    "    \\begin{equation*}\n",
    "        p( U = b) = \\sum_{i=1}^3 p( U = b, C = i) = \\sum_{i=1}^{3} p( U = b \\vert  C = i) \\cdot p( C = i)\\,.\n",
    "    \\end{equation*}\n",
    "\n",
    "    With the following probabilities\n",
    "    \\begin{align*}\n",
    "            p( C = i) &= \\frac13 &&\\text{(all cards are equally likely)}\\\\\n",
    "            p( U = b \\vert C = 1) &= 1&&\\text{(the first card has two black sides)}\\\\\n",
    "            p( U = b \\vert C = 2) &= 0&&\\text{(the second card has no black side)}\\\\\n",
    "            p( U = b \\vert C = 3) &= \\frac{1}{2}&&\\text{(the third card has one black side)}\n",
    "    \\end{align*}\n",
    "    we find\n",
    "\n",
    "    \\begin{equation*}\n",
    "        p( U=b) = \\frac{1}{3} \\cdot 1 + \\frac{1}{3}\\cdot 0 + \\frac{1}{3} \\cdot\\frac{1}{2} = \\frac{1}{2}\\,.\n",
    "    \\end{equation*}\n",
    "\n",
    "    We do not have to redo the calculation for $p( U=w)$, because the scenario is totally symmetric, meaning there is no difference between the probabilities for either color.\n",
    "- b) Now we are asked to compute $p(D=b\\vert U = b)$. First note that there is only card $1$ for which this is possible, so we can equivalently compute $p( C = 1 \\vert U =b)$\n",
    "\n",
    "    \\begin{equation*}\n",
    "        p( C = 1 \\vert  U = b ) = \\frac{p(C = 1, U = b) }{p( U = b)}\\,.\n",
    "    \\end{equation*}\n",
    "\n",
    "    We computed the denominator in (a), and the numerator is simply $p( C = 1) = \\frac{1}{3}$ as card one has two black sides.\n",
    "    Therefore, we find\n",
    "\n",
    "    \\begin{equation*}\n",
    "       p( D= b \\vert U = b)  = \\frac{2}{3}\\,.\n",
    "    \\end{equation*}\n",
    "\n",
    "    That fact that it is large than $\\frac{1}{2}$ here shows that a black side up is more likely to happen with card 1 than 3.\n",
    "    In fact, from the 3 black sides that could be facing up (two from card 1, one from card 3), two thirds belong to card 1 which corresponds to the probability we wanted to compute.\n",
    "\n",
    "- c) Due to the symmetry of black and white in the problem, we can also find the probability $p(D = w \\vert  U = b)$. As this is the complement to the event considered in (b), we find\n",
    "\n",
    "    \\begin{equation*}\n",
    "        p( D= b \\vert U = w)  = p( D= w \\vert U = b) = 1 -  p( D= b \\vert U = b) = \\frac{1}{3}\\,.\n",
    "    \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.2 Entropy and Kullback Leibler Divergence\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "a) Compute the Entropy of the normal distribution\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal N(x; \\mu,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\,.\n",
    "\\end{equation*}\n",
    "\n",
    "b) Find the Kullback-Leibler divergence between two Gaussian distributions, i.e.\n",
    "\n",
    "\\begin{equation*}\n",
    "D_{KL}\\left( \\mathcal N(\\mu_1, \\sigma_1^2) \\vert\\vert \\mathcal N(\\mu_2, \\sigma_2^2) \\right)\\,.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:** **TODO**\n",
    "\n",
    "- a) The entropy of a Gaussian can be computed without performing any integrals directly:\n",
    "\n",
    "    \\begin{align*}\n",
    "\tH\\left[ \\mathcal{N} \\right] &= - \\intop_{-\\infty}^{\\infty} \\log\\left( N(x; \\mu,\\sigma^2) \\right)  \\cdot   N(x; \\mu,\\sigma^2) \\,\\mathrm{d} x\\\\\n",
    "\t&= - \\intop_{-\\infty}^{\\infty} \\left[ -\\frac{\\left( x - \\mu \\right)^2}{2\\sigma^2} - \\frac{1}{2} \\log\\left( 2\\pi\\sigma^2 \\right) \\right] \\cdot  N(x; \\mu,\\sigma^2) \\,\\mathrm{d} x\\\\\n",
    "\t&= \\frac{1}{2\\sigma^2} \\underbrace{\\intop_{-\\infty}^{\\infty}\\left( x - \\mu \\right)^2  \\cdot  N(x; \\mu,\\sigma^2) \\,\\mathrm{d} x}_{ = \\sigma^2}\n",
    "\t+ \\frac{1}{2} \\log\\left( 2\\pi\\sigma^2 \\right) \\underbrace{\\intop_{-\\infty}^{\\infty}  N(x; \\mu,\\sigma^2) \\,\\mathrm{d} x}_{=1}\\\\\n",
    "\t&= \\frac{1}{2} + \\frac{1}{2} \\log\\left( 2\\pi\\sigma^2 \\right) = \\frac{1}{2} \\log\\left( 2\\pi\\mathrm{e} \\sigma^2 \\right) \n",
    "    \\end{align*}\n",
    "\n",
    "- b) Let's first simplify the expression for the Kullback-Leibler divergence, by introducing the notation $\\mathcal{N}_1(x) = \\mathcal{N}(x; \\mu_1, \\sigma_1^2)$ and $\\mathcal{N}_2(x) = \\mathcal{N}(x; \\mu_2, \\sigma_2^2)$:\n",
    "    \\begin{align}\n",
    "    D_{KL} \\left( \\mathcal{N}_1 \\vert\\vert \\mathcal{N}_2 \\right) &=\n",
    "\t\\intop_{-\\infty}^\\infty \\mathcal{N}_1(x) \\cdot \\log\\left( \\frac{\\mathcal{N}_1(x)}{\\mathcal{N}_2(x)} \\right)\\,\\mathrm{d}x \\nonumber \\\\\n",
    "\t&= -H\\left[ \\mathcal{N}_1\\right] - \\intop_{-\\infty}^\\infty \\log\\left(\\mathcal{N}_2(x)\\right) \\mathcal{N}_1(x)\\,\\mathrm{d} x \\nonumber\\\\\n",
    "\t&= -\\frac{\\log\\left( 2\\pi\\mathrm{e} \\sigma_1^2 \\right)}{2} \n",
    "\t-\\intop_{-\\infty}^\\infty \\left( -\\frac{\\log(2\\pi\\sigma_2^2)}{2} - \\frac{(x-\\mu_2)^2}{2\\sigma_2^2}\\right) \\cdot \\mathcal{N}_1(x) \\,\\mathrm{d} x \\nonumber\\\\\n",
    "\t&= -\\frac{\\log\\left( 2\\pi\\mathrm{e} \\sigma_1^2 \\right)}{2} \n",
    "\t+\\frac{\\log(2\\pi\\sigma_2^2)}{2}\n",
    "\t+\\intop_{-\\infty}^\\infty \\frac{(x-\\mu_2)^2}{2\\sigma_2^2} \\cdot \\mathcal{N}_1(x) \\,\\mathrm{d} x \\nonumber\\\\\n",
    "\t&= \\log\\left( \\frac{\\sigma_2}{\\sigma_1} \\right) - \\frac{1}{2} \n",
    "\t+\\intop_{-\\infty}^\\infty \\frac{(x-\\mu_2)^2}{2\\sigma_2^2} \\cdot \\mathcal{N}_1(x) \\,\\mathrm{d} x\n",
    "\t\\label{eq:kl1}%\n",
    "    \\end{align}\n",
    "    \n",
    "    For space reasons, we continue to compute only the last integral\n",
    "\n",
    "    \\begin{align*}\n",
    "    \\intop_{-\\infty}^\\infty (x-\\mu_2)^2 \\cdot \\mathcal{N}(x;\\mu_1,\\sigma_1^2) \\,\\mathrm{d} x &=\\intop_{-\\infty}^\\infty \n",
    "\t\\underbrace{(x-\\mu_1 + \\mu_1 -\\mu_2)^2}_{ = (x-\\mu_1)^2\t+ (\\mu_1 - \\mu_2)^2 \\atop + 2 (x-\\mu_1)(\\mu_1-\\mu_2)}\n",
    "\t\\cdot \\mathcal{N}(x;\\mu_1,\\sigma_1^2) \\,\\mathrm{d} x \\\\\n",
    "    &= \\underbrace{\\intop_{-\\infty}^\\infty (x-\\mu_1)^2\\mathcal{N}(x;\\mu_1,\\sigma_1^2)\\,\\mathrm{d} x}_{=\\sigma_1^2} \\\\\n",
    "     &\\qquad+ (\\mu_1-\\mu_2)^2 \\underbrace{\\intop_{-\\infty}^\\infty \\mathcal{N}(x;\\mu_1,\\sigma_1^2)\\,\\mathrm{d} x}_{=1}\\\\\n",
    "     &\\qquad \\qquad + 2(\\mu_1-\\mu_2) \\underbrace{\\intop_{-\\infty}^\\infty (x-\\mu_1) \\mathcal{N}(x;\\mu_1,\\sigma_1^2)\\,\\mathrm{d} x}_{= 0}\\\\\n",
    "    &= \\sigma_1^2 + (\\mu_1 - \\mu_2)^2\t\n",
    "    \\end{align*}\n",
    "    \n",
    "    Plugging this back into the previous equation yields\n",
    "    \\begin{equation*}\n",
    "    D_{KL} \\left( \\mathcal{N}(\\mu_1, \\sigma_1^2) \\vert\\vert \\mathcal{N}(\\mu_2,\\sigma_2^2)\\right) = \\log\\left( \\frac{\\sigma_2}{\\sigma_1} \\right) - \\frac{1}{2} \n",
    "+ \\frac{\\sigma_1^2 + \\left( \\mu_1-\\mu_2 \\right)^2}{2\\sigma_2^2}\\,.\n",
    "  \\end{equation*}\n",
    "  \n",
    "  Note that the divergence is 0, if $\\mu_1 = \\mu_2$ and $\\sigma_1 = \\sigma_2$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.3 Distributions and CLT\n",
    "\n",
    "The central limit theorem states that for i.i.d. random samples $\\{X_i\\}$ from an (almost) arbitrary distribution with given mean $\\mu$ and variance $\\sigma^2$, the mean $\\frac{1}{n}\\sum_{i=1}^n X_i$ follows approximately a normal distribution. More precisely, it reads\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\frac{1}{n} \\sum_{i=1}^n X_i \\xrightarrow{n\\to\\infty} \\mathcal N(\\mu, \\frac{\\sigma^2}{n})\\,.\n",
    "\\end{equation*}\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- a) Draw $n=1,16,64, 1024$ samples from the distributions below for $1024$ times (each). Draw for each (n, distribution) pair a histogram over the sample mean. Include the corresponding normal distribution (pdf) in the plot.\n",
    "\n",
    "    - the exponential distribution $p(X) = \\lambda \\mathrm{e}^{-\\lambda X}$\n",
    "    - the Gaussian/normal distribution \n",
    "- b) Assume, that you can only sample from uniform distributions. Implement functions to sample from an approximated standard normal distribution and an approximated normal distribution. Plot the distributions in comparison to the numpy implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# a)\n",
    "## START TODO ########################\n",
    "n_repetitions = 1024\n",
    "sample_sizes = (1, 16, 64, 1024)\n",
    "distributions = (lambda size: np.random.exponential(1, size),\n",
    "                 lambda size: np.random.normal(1, 1, size))\n",
    "\n",
    "subplot = (len(sample_sizes), len(distributions), 1)\n",
    "fig = plt.figure(figsize=(8, 12))\n",
    "\n",
    "for n in sample_sizes:\n",
    "    for dist in distributions:\n",
    "        plt.subplot(*subplot)\n",
    "        subplot = (*subplot[:-1], subplot[-1] + 1)\n",
    "\n",
    "        samples = dist((n_repetitions, n))\n",
    "        mean = np.mean(samples, axis=1)\n",
    "        _, bins, _ = plt.hist(mean, label=n, normed=True)\n",
    "        x = np.linspace(bins[0], bins[-1], 100)\n",
    "        pdf = stats.norm.pdf(x, 1, 1/np.sqrt(n))\n",
    "        plt.plot(x, pdf)\n",
    "        plt.legend()\n",
    "        \n",
    "fig.axes[0].set_title(\"exponential\")\n",
    "fig.axes[1].set_title(\"normal\")\n",
    "plt.show()\n",
    "## END TODO ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "def std_normal(n_samples: int = 1) -> np.ndarray:\n",
    "    \"\"\" Sample from a standard normal distribution. \n",
    "        The normal distribution is approximated via a uniform dist.\n",
    "    \n",
    "        n_samples: number of samples of a standard normal distribution\n",
    "        return:    samples. length = n_samples\n",
    "    \"\"\"\n",
    "    ## START TODO ########################\n",
    "    n_uniform_samples = 1024\n",
    "    \n",
    "    # var_normal = var_uniform / n = 1       | see CLT above\n",
    "    # var_uniform = 1/12 * (right - left)^2  | see e.g. Wikipedia\n",
    "    #             = 1/12 * (2 * scale)^2     | symmetric uniform\n",
    "    # insert var_uniform in var_normal and solve for scale\n",
    "    \n",
    "    scale = np.sqrt(n_uniform_samples) * np.sqrt(3)\n",
    "    samples = np.random.uniform(-scale, scale, size=(n_samples, n_uniform_samples))\n",
    "    return np.mean(samples, axis=1)\n",
    "    ## END TODO ########################\n",
    "    # return value has len n_samples\n",
    "\n",
    "def normal(loc: float = 0.0, scale: float = 1.0, n_samples: int = 1) -> np.ndarray:\n",
    "    \"\"\" Sample from a normal distribution.\n",
    "        The normal distribution is approximated via a uniform dist.\n",
    "    \n",
    "        loc: mean of the distribution\n",
    "        scale: standard deviation spread of the distribution.\n",
    "        \n",
    "        n_samples: number of samples\n",
    "        return:    samples. length = n_samples\n",
    "    \"\"\"\n",
    "    ## START TODO ########################\n",
    "    return loc + scale * std_normal(n_samples)\n",
    "    ## END TODO ########################\n",
    "\n",
    "\n",
    "n_samples = 10000\n",
    "bins = 20\n",
    "\n",
    "# plot histograms for standard normal with std_normal and np.random.normal\n",
    "## START TODO ########################\n",
    "plt.hist(std_normal(n_samples), bins, label='custom', histtype='stepfilled')\n",
    "plt.hist(np.random.normal(0, 1, n_samples), bins, label='numpy', histtype='step')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "## END TODO ########################\n",
    "\n",
    "# plot histograms for N(mean=-10, std=3) with normal and np.random.normal\n",
    "## START TODO ########################\n",
    "plt.hist(normal(-10, 3, n_samples), bins, label='custom', histtype='stepfilled')\n",
    "plt.hist(np.random.normal(-10, 3, n_samples), bins, label='numpy', histtype='step')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "## END TODO ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 7.1\n",
    "\n",
    "* **Major Problems: ** TODO\n",
    "* **Helpful? ** TODO\n",
    "* **Duration (hours): ** TODO\n",
    "* **Other feedback: **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Autoencoder\n",
    "\n",
    "Let's now implement our own variational auto encoder (VAE). The VAE is described in chapter 20.10.3 of the Deep Learning Book and you can find the original paper [here](https://arxiv.org/pdf/1312.6114.pdf).\n",
    "\n",
    "Compared to a \"standard\" auto encoder, a VAE learns to approximate the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])),\n",
    "    batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_space_size=2):\n",
    "        \"\"\"Variational auto encoder.\n",
    "        \n",
    "        Create the VAE with the MLP encoder having one hidden layer\n",
    "        and two output layers (mean and log of the variance).\n",
    "        Use a hidden size of 100 and each output size of `latent_space_size`.\n",
    "        \n",
    "        The decoder should be as powerful as the encoder.\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        super(VAE, self).__init__()\n",
    "        self.latent_space_size = latent_space_size\n",
    "        self.fc1 = nn.Linear(784, 100)\n",
    "        self.fc_mu = nn.Linear(100, latent_space_size)\n",
    "        self.fc_logvar = nn.Linear(100, latent_space_size)\n",
    "        self.fc3 = nn.Linear(latent_space_size, 100)\n",
    "        self.fc4 = nn.Linear(100, 784)\n",
    "        ## END TODO ########################\n",
    "\n",
    "    def encode(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Encode x to distribution.\n",
    "        \n",
    "        Compute the hidden representation. Estimate and return the mean\n",
    "        and the log of the variance from the hidden representation. Use\n",
    "        relu as activation function for the hidden representation.\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        return self.fc_mu(h1), self.fc_logvar(h1)\n",
    "        ## END TODO ########################\n",
    "\n",
    "    def reparameterize(self, mu, logvar) -> torch.Tensor:\n",
    "        \"\"\"Sample from the estimated distribution.\n",
    "        \n",
    "        In training mode, return a sample from N(mu, sigma^2).\n",
    "        In evaluation mode, just return the mean.\n",
    "        \n",
    "        Hint: You estimate the log of the variance, so you need to transform\n",
    "        this to std deviation first, as torch.randn\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        if self.training:\n",
    "            # we estimate log(var)\n",
    "            #       stddev = sqrt(var)\n",
    "            # <==>  stddev = e^log(var^0.5)\n",
    "            # <==>  stddev = e^0.5*log(var)\n",
    "            std = torch.exp(0.5*logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps * std + mu\n",
    "        else:\n",
    "            return mu\n",
    "        ## End TODO ########################\n",
    "\n",
    "    def decode(self, z) -> torch.Tensor:\n",
    "        \"\"\" Decode the latent representation.\n",
    "        \n",
    "        Compute the hidden representation from the latent space z,\n",
    "        use relu as activation. Then reconstruct the signal from\n",
    "        the hidden representation, using a sigmoid as activation.\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "        ## END TODO ########################\n",
    "\n",
    "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Encode, reparameterize and decode.\n",
    "        \n",
    "        Returns:\n",
    "            (The decoded result, mu, logvar)\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "        ## END TODO ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(x_reconstructed, x, mu, logvar, kl_loss_weight=1):\n",
    "    ## START TODO ########################\n",
    "    # compute the binary cross entropy between x and the reconstructed x\n",
    "    rec_loss = F.binary_cross_entropy(x_reconstructed, x.view(-1, 784), reduction='sum')\n",
    "    # Here your results from 7.1.2 come into play - The first distribution is\n",
    "    # the N(mu, sigma^2) and the second is N(0, 1). Plugging this into\n",
    "    # the result from 7.1.2 yields\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # you can also check Appendix B from the VAE paper:\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    kl_div_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    ## END TODO ########################\n",
    "    return rec_loss + kl_div_loss * kl_loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, optimizer, kl_loss_weight=1, save_images=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(reconstructed_batch, data, mu, logvar, kl_loss_weight)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if save_images:\n",
    "        n = min(data.size(0), 8)\n",
    "        training_image = torch.cat([data[:n]])\n",
    "        save_image(training_image.cpu(),\n",
    "                   'results/training_samples_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    print('====> Epoch: {} Average train set loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(model, epoch, kl_loss_weight=1, save_images=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            reconstructed_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(reconstructed_batch, data, mu, logvar, kl_loss_weight).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      reconstructed_batch.view(128, 1, 28, 28)[:n]])\n",
    "                if not save_images:\n",
    "                    continue\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Average test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_size = 2\n",
    "model = VAE(latent_space_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "if not os.path.isdir('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "for epoch in range(1, 15):\n",
    "    train(model, epoch, optimizer, save_images=True)\n",
    "    test(model, epoch, save_images=True)\n",
    "    with torch.no_grad():\n",
    "        ## START TODO ########################\n",
    "        # sample 64 images (generate a random latent_space and decode)\n",
    "        sample = torch.randn(64, latent_space_size).to(device)\n",
    "        sample = model.decode(sample).cpu()\n",
    "        ## END TODO ########################\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the latent space\n",
    "\n",
    "One of the easiest ways to visualize the latent space is to limit the size to two dimensions (which of course might not always capture the data well) and to sample from the model over a 2D grid. This is what we're going to do below. Also, we can nicely visualize the concept of a VAE. We plot the mean over the estimated means for each class (the numbers 0 to 9) and the mean estimated standard deviation for each class (the blue crosses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu_logvar(model: nn.Module) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute the model's prediction of mean and logvar for all\n",
    "       images in the MNIST test set.\n",
    "    \"\"\"\n",
    "    mus, logvars = [], []\n",
    "    # encode each image of the test_loader\n",
    "    ## START TODO ########################\n",
    "    for img_batch, label_batch in test_loader:\n",
    "        mu, logvar = model.encode(img_batch.view(-1, 784))\n",
    "        mus.append(mu)\n",
    "        logvars.append(logvar)\n",
    "    ## END TODO ########################\n",
    "    return torch.cat(mus), torch.cat(logvars)\n",
    "\n",
    "    \n",
    "def sample_on_grid(latent_min: Tuple[int, int], latent_max: Tuple[int, int], \n",
    "                   model: nn.Module, grid_size: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ## START TODO ########################\n",
    "    # create a grid in two directions from latent_min to latent_max, using\n",
    "    # the respective dimension\n",
    "    x1 = torch.linspace(float(latent_min[0]), float(latent_max[0]), grid_size)\n",
    "    x2 = torch.linspace(float(latent_min[1]), float(latent_max[1]), grid_size)\n",
    "    latent_grid = torch.stack(torch.meshgrid((x1, x2))).transpose(0, 2).reshape((-1, 2))\n",
    "    # use the created grid as latent variables and run them through the decoder\n",
    "    decoded = model.decode(latent_grid).view(-1, 1, 28, 28).detach().numpy()\n",
    "    ## END TODO ########################\n",
    "    step_size = np.array((x1[1] - x1[0], x2[1] - x2[0]))\n",
    "    return decoded.reshape(grid_size, grid_size, 28, 28), step_size\n",
    "\n",
    "\n",
    "def plot_latent_space(model: nn.Module):\n",
    "    model.eval()\n",
    "    mus, logvars = get_mu_logvar(model)\n",
    "    mus = mus.detach().numpy()\n",
    "    stddev = np.exp(0.5*logvars.detach().numpy())\n",
    "\n",
    "    labels = torch.cat([label for _, label in test_loader]).detach().numpy()\n",
    "    num_samples = labels.shape[0]\n",
    "    class_mu_0 = np.bincount(labels, weights=mus[:, 0]) / np.bincount(labels)\n",
    "    class_mu_1 = np.bincount(labels, weights=mus[:, 1]) / np.bincount(labels)\n",
    "    mean_mu = np.stack((class_mu_0, class_mu_1))\n",
    "    print(\"Per class mean of estimated mean:\\n\", mean_mu)\n",
    "\n",
    "    class_stddev_0 = np.bincount(labels, weights=stddev[:, 0]) / np.bincount(labels)\n",
    "    class_stddev_1 = np.bincount(labels, weights=stddev[:, 1]) / np.bincount(labels)\n",
    "    mean_stddev = np.stack((class_stddev_0, class_stddev_1))\n",
    "    print(\"Per class mean of estimated std deviation:\\n\", mean_stddev)\n",
    "\n",
    "    # get the minimum and maximum values of the latent space means\n",
    "    # we will use this as boundaries from which we sample in the latent space\n",
    "    latent_min = np.min(mean_mu - mean_stddev * 2, axis=1)\n",
    "    latent_max = np.max(mean_mu + mean_stddev * 2, axis=1)\n",
    "\n",
    "    # Produce a 20x20 2D grid of evenly spaced values between latent_min ant latent_max\n",
    "    decoded, step_size = sample_on_grid(latent_min, latent_max, model, grid_size=20)\n",
    "    \n",
    "    # visualize the decoded images\n",
    "    # using reshape and np.concatenate and calling imshow only once\n",
    "    # is a lot faster than creating a subplot and calling imshow 400 times.\n",
    "    # It's also necessary to be able to plot the means\n",
    "    block_img = np.concatenate(np.concatenate(decoded, axis=1), axis=1)\n",
    "    f = plt.figure(figsize=(12,12))\n",
    "    plt.imshow(block_img, cmap='Greys')\n",
    "    plt.axis('off');\n",
    "\n",
    "    # visualize the mean mu and the mean standard deviation of each class\n",
    "    # scale the mean accordingly, as the plot's axes represent pixels\n",
    "    scale = (28 / step_size)\n",
    "\n",
    "    mean_mu_scaled = 28/2 + scale.reshape(2, 1) * (mean_mu - latent_min.reshape(2,1))\n",
    "    # we scale the stddev by 2 so the arrows are extended a bit more\n",
    "    mean_stddev_scaled = mean_stddev * scale.reshape(2, 1) * 2\n",
    "    # plot the std deviation\n",
    "    plt.errorbar(mean_mu_scaled[0], mean_mu_scaled[1],\n",
    "                 yerr=mean_stddev_scaled[1], xerr=mean_stddev_scaled[0],\n",
    "                 linestyle='None')\n",
    "    # plot the means\n",
    "    plt.scatter(mean_mu_scaled[0], mean_mu_scaled[1])\n",
    "\n",
    "    for i, txt in enumerate([str(i) for i in range(0,10)]):\n",
    "        plt.annotate(txt, (mean_mu_scaled[0][i] + 3, mean_mu_scaled[1][i] - 3),\n",
    "                     color='blue',\n",
    "                     bbox={'facecolor':'white', 'alpha':0.9, 'pad':1, 'edgecolor':'none'})\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_space(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the influence of the KL-divergence in the loss\n",
    "\n",
    "Let's now investigate the influence of the KL-divergence by training\n",
    "- one model where we weight the KL-divergence part of the loss by a factor of 30 and\n",
    "- one model where we remove it (weight 0).\n",
    "\n",
    "Visualize the latent space as we did above.\n",
    "\n",
    "**ToDo** What do you observe? How can these results be explained? What is the role of the KL divergence term?\n",
    "\n",
    "Assigning a large weight to the KL divergence means we almost exlusively pull the learned distribution towards the Normal distribution with mean 0 and std deviation of 1. This trains the network to always estimate something close  to mean 0 and stddev 1, which prevents learning of a meaningful latent space from which the original signal can be reconstructed.\n",
    "\n",
    "Training the Autoencoder without KL divergence leads to a latent space with larger differences between latent distribution means and std deviations, making reconstruction more difficult.\n",
    "\n",
    "The KL divergence therefore acts as a regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START TODO ########################\n",
    "# model_large_kl = \n",
    "latent_space_size = 2\n",
    "kl_loss_weight = 30\n",
    "model_large_kl = VAE(latent_space_size).to(device)\n",
    "optimizer = optim.Adam(model_large_kl.parameters(), lr=1e-3)\n",
    "for epoch in range(1, 15):\n",
    "    train(model_large_kl, epoch, optimizer, kl_loss_weight)\n",
    "    test(model_large_kl, epoch, kl_loss_weight)\n",
    "## END TODO ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(model_large_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START TODO ########################\n",
    "# model_no_kl =\n",
    "latent_space_size = 2\n",
    "kl_loss_weight = 0\n",
    "model_no_kl = VAE(latent_space_size).to(device)\n",
    "optimizer = optim.Adam(model_no_kl.parameters(), lr=1e-3)\n",
    "for epoch in range(1, 15):\n",
    "    train(model_no_kl, epoch, optimizer, kl_loss_weight)\n",
    "    test(model_no_kl, epoch, kl_loss_weight)\n",
    "## END TODO ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(model_no_kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 7.2\n",
    "\n",
    "* **Major Problems: ** TODO\n",
    "* **Helpful? ** TODO\n",
    "* **Duration (hours): ** TODO\n",
    "* **Other feedback: **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
