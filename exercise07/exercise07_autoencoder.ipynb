{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seventh Exercise (Chapter 3 & Chapter 14)\n",
    "\n",
    "In the first part we will recap basics of probability theory. \n",
    "\n",
    "In the second part, we're going to implement an autoencoder and explore the learned latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Probability\n",
    "\n",
    "Solve the first two exercises mathematically. Answer using markdown and latex in this notebook. Please provide intermediate steps and explanations. \n",
    "\n",
    "### 7.1.1 Bertrand's Box Paradox\n",
    "\n",
    "Consider a box containing the following 3 cards\n",
    "\n",
    "- one with two black sides\n",
    "- one with two white sides\n",
    "- one with a black and a white side\n",
    "\n",
    "From the three cards, one is drawn at random and put on the table. You can only see the side facing up.\n",
    "\n",
    "**Questions:** \n",
    "\n",
    "a) What are the probabilities that the card on the table shows a black side? What are the probabilities it shows a white side?   \n",
    "b) If we draw a card and it shows black, compute the probability that the other side of the card is also black.  \n",
    "c) Find the the probability that the other side of the card is black if the card shows a white side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:** **TODO**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.2 Entropy and Kullback Leibler Divergence\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "a) Compute the Entropy of the normal distribution\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal N(x; \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\mathrm{e}^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\\,.\n",
    "\\end{equation*}\n",
    "\n",
    "b) Find the Kullback-Leibler divergence between two Gaussian distributions, i.e.\n",
    "\n",
    "\\begin{equation*}\n",
    "D_{KL}\\left( \\mathcal N(\\mu_1, \\sigma_1) \\vert\\vert \\mathcal N(\\mu_2, \\sigma_2) \\right)\\,.\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:** **TODO**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1.3 Distributions and CLT\n",
    "\n",
    "The central limit theorem states that for i.i.d. random samples $\\{X_i\\}$ from an (almost) arbitrary distribution with given mean $\\mu$ and variance $\\sigma^2$, the mean $\\frac{1}{n}\\sum_{i=1}^n X_i$ follows approximately a normal distribution. More precisely, it reads\n",
    "\n",
    "\\begin{equation*}\n",
    "\t\\frac{1}{n} \\sum_{i=1}^n X_i \\xrightarrow{n\\to\\infty} \\mathcal N(\\mu, \\frac{\\sigma^2}{n})\\,.\n",
    "\\end{equation*}\n",
    "\n",
    "**Questions:**\n",
    "\n",
    "- a) Draw $n=1,16,64, 1024$ samples from the distributions below for $1024$ times (each). Draw for each (n, distribution) pair a histogram over the sample mean. Include the corresponding normal distribution (pdf) in the plot.\n",
    "\n",
    "    - the exponential distribution $p(X) = \\lambda \\mathrm{e}^{-\\lambda X}$\n",
    "    - the Gaussian/normal distribution \n",
    "- b) Assume, that you can only sample from uniform distributions. Implement functions to sample from an approximated standard normal distribution and an approximated normal distribution. Plot the distributions in comparison to the numpy implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# a)\n",
    "## START TODO ########################\n",
    "\n",
    "## END TODO ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b)\n",
    "def std_normal(n_samples: int = 1) -> np.ndarray:\n",
    "    \"\"\" Sample from a standard normal distribution. \n",
    "        The normal distribution is approximated via a uniform dist.\n",
    "    \n",
    "        n_samples: number of samples of a standard normal distribution\n",
    "        return:    samples. length = n_samples\n",
    "    \"\"\"\n",
    "    ## START TODO ########################\n",
    "\n",
    "    ## END TODO ########################\n",
    "    # return value has len n_samples\n",
    "\n",
    "def normal(loc: float = 0.0, scale: float = 1.0, n_samples: int = 1) -> np.ndarray:\n",
    "    \"\"\" Sample from a normal distribution.\n",
    "        The normal distribution is approximated via a uniform dist.\n",
    "    \n",
    "        loc: mean of the distribution\n",
    "        scale: standard deviation spread of the distribution.\n",
    "        \n",
    "        n_samples: number of samples\n",
    "        return:    samples. length = n_samples\n",
    "    \"\"\"\n",
    "    ## START TODO ########################\n",
    "    pass\n",
    "    ## END TODO ########################\n",
    "\n",
    "\n",
    "n_samples = 10000\n",
    "bins = 20\n",
    "\n",
    "# plot histograms for standard normal with std_normal and np.random.normal\n",
    "## START TODO ########################\n",
    "\n",
    "## END TODO ########################\n",
    "\n",
    "# plot histograms for N(mean=-10, std=3) with normal and np.random.normal\n",
    "## START TODO ########################\n",
    "\n",
    "## END TODO ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 7.1\n",
    "\n",
    "* **Major Problems: ** TODO\n",
    "* **Helpful? ** TODO\n",
    "* **Duration (hours): ** TODO\n",
    "* **Other feedback: **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Autoencoder\n",
    "\n",
    "Let's now implement our own variational auto encoder (VAE). The VAE is described in chapter 20.10.3 of the Deep Learning Book and you can find the original paper [here](https://arxiv.org/pdf/1312.6114.pdf).\n",
    "\n",
    "Compared to a \"standard\" auto encoder, a VAE learns to approximate the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import argparse\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])),\n",
    "    batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_space_size=2):\n",
    "        \"\"\"Variational auto encoder.\n",
    "        \n",
    "        Create the VAE with the MLP encoder having one hidden layer\n",
    "        and two output layers (mean and log of the variance).\n",
    "        Use a hidden size of 100 and each output size of `latent_space_size`.\n",
    "        \n",
    "        The decoder should be as powerful as the encoder.\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        pass\n",
    "        ## END TODO ########################\n",
    "\n",
    "    def encode(self, x) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" Encode x to distribution.\n",
    "        \n",
    "        Compute the hidden representation. Estimate and return the mean\n",
    "        and the log of the variance from the hidden representation. Use\n",
    "        relu as activation function for the hidden representation.\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        raise NotImplementedError\n",
    "        ## END TODO ########################\n",
    "\n",
    "    def reparameterize(self, mu, logvar) -> torch.Tensor:\n",
    "        \"\"\"Sample from the estimated distribution.\n",
    "        \n",
    "        In training mode, return a sample from N(mu, logvar).\n",
    "        In evaluation mode, just return the mean.\n",
    "        \n",
    "        Hint: You estimate the log of the variance, so you need to transform\n",
    "        this to std deviation first, as torch.randn\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        raise NotImplementedError\n",
    "        ## End TODO ########################\n",
    "\n",
    "    def decode(self, z) -> torch.Tensor:\n",
    "        \"\"\" Decode the latent representation.\n",
    "        \n",
    "        Compute the hidden representation from the latent space z,\n",
    "        use relu as activation. Then reconstruct the signal from\n",
    "        the hidden representation, using a sigmoid as activation.\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        raise NotImplementedError\n",
    "        ## END TODO ########################\n",
    "\n",
    "    def forward(self, x) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Encode, reparameterize and decode.\n",
    "        \n",
    "        Returns:\n",
    "            (The decoded result, mu, logvar)\n",
    "        \"\"\"\n",
    "        ## START TODO ########################\n",
    "        raise NotImplementedError\n",
    "        ## END TODO ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruction + KL divergence losses summed over all elements and batch\n",
    "def loss_function(x_reconstructed, x, mu, logvar, kl_loss_weight=1):\n",
    "    ## START TODO ########################\n",
    "    # compute the binary cross entropy between x and the reconstructed x\n",
    "    \n",
    "    # rec_loss = \n",
    "    \n",
    "    # Here your results from 7.1.2 come into play - The first distribution is\n",
    "    # the N(mu, logvar) and the second is N(0, 1). Plugging this into\n",
    "    # the result from 7.1.2 yields\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    # you can also check Appendix B from the VAE paper:\n",
    "    # https://arxiv.org/abs/1312.6114\n",
    "    \n",
    "    # kl_div_loss = \n",
    "    \n",
    "    ## END TODO ########################\n",
    "    return rec_loss + kl_div_loss * kl_loss_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, optimizer, kl_loss_weight=1, save_images=False):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed_batch, mu, logvar = model(data)\n",
    "        loss = loss_function(reconstructed_batch, data, mu, logvar, kl_loss_weight)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    if save_images:\n",
    "        n = min(data.size(0), 8)\n",
    "        training_image = torch.cat([data[:n]])\n",
    "        save_image(training_image.cpu(),\n",
    "                   'results/training_samples_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    print('====> Epoch: {} Average train set loss: {:.4f}'.format(\n",
    "          epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "def test(model, epoch, kl_loss_weight=1, save_images=False):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            reconstructed_batch, mu, logvar = model(data)\n",
    "            test_loss += loss_function(reconstructed_batch, data, mu, logvar, kl_loss_weight).item()\n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n],\n",
    "                                      reconstructed_batch.view(128, 1, 28, 28)[:n]])\n",
    "                if not save_images:\n",
    "                    continue\n",
    "                save_image(comparison.cpu(),\n",
    "                         'results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Average test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_space_size = 2\n",
    "model = VAE(latent_space_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "if not os.path.isdir('results'):\n",
    "    os.mkdir('results')\n",
    "\n",
    "for epoch in range(1, 15):\n",
    "    train(model, epoch, optimizer, save_images=True)\n",
    "    test(model, epoch, save_images=True)\n",
    "    with torch.no_grad():\n",
    "        ## START TODO ########################\n",
    "        # sample 64 images (generate a random latent_space and decode)\n",
    "        \n",
    "        #sample = ...\n",
    "        ## END TODO ########################\n",
    "        save_image(sample.view(64, 1, 28, 28),\n",
    "                   'results/sample_' + str(epoch) + '.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the latent space\n",
    "\n",
    "One of the easiest ways to visualize the latent space is to limit the size to two dimensions (which of course might not always capture the data well) and to sample from the model over a 2D grid. This is what we're going to do below. Also, we can nicely visualize the concept of a VAE. We plot the mean over the estimated means for each class (the numbers 0 to 9) and the mean estimated standard deviation for each class (the blue crosses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mu_logvar(model: nn.Module) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Compute the model's prediction of mean and logvar for all\n",
    "       images in the MNIST test set.\n",
    "    \"\"\"\n",
    "    mus, logvars = [], []\n",
    "    # encode each image of the test_loader\n",
    "    ## START TODO ########################\n",
    "    \n",
    "    ## END TODO ########################\n",
    "    return torch.cat(mus), torch.cat(logvars)\n",
    "\n",
    "    \n",
    "def sample_on_grid(latent_min: Tuple[int, int], latent_max: Tuple[int, int], \n",
    "                   model: nn.Module, grid_size: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    ## START TODO ########################\n",
    "    # create a grid in two directions from latent_min to latent_max, using\n",
    "    # the respective dimension\n",
    "    # x1 = ...\n",
    "    # x2 = ...\n",
    "    # use the created grid as latent variables and run them through the decoder\n",
    "\n",
    "    ## END TODO ########################\n",
    "    step_size = np.array((x1[1] - x1[0], x2[1] - x2[0]))\n",
    "    return decoded.reshape(grid_size, grid_size, 28, 28), step_size\n",
    "\n",
    "\n",
    "def plot_latent_space(model: nn.Module):\n",
    "    model.eval()\n",
    "    mus, logvars = get_mu_logvar(model)\n",
    "    mus = mus.detach().numpy()\n",
    "    stddev = np.exp(0.5*logvars.detach().numpy())\n",
    "\n",
    "    labels = torch.cat([label for _, label in test_loader]).detach().numpy()\n",
    "    num_samples = labels.shape[0]\n",
    "    class_mu_0 = np.bincount(labels, weights=mus[:, 0]) / np.bincount(labels)\n",
    "    class_mu_1 = np.bincount(labels, weights=mus[:, 1]) / np.bincount(labels)\n",
    "    mean_mu = np.stack((class_mu_0, class_mu_1))\n",
    "    print(\"Per class mean of estimated mean:\\n\", mean_mu)\n",
    "\n",
    "    class_stddev_0 = np.bincount(labels, weights=stddev[:, 0]) / np.bincount(labels)\n",
    "    class_stddev_1 = np.bincount(labels, weights=stddev[:, 1]) / np.bincount(labels)\n",
    "    mean_stddev = np.stack((class_stddev_0, class_stddev_1))\n",
    "    print(\"Per class mean of estimated std deviation:\\n\", mean_stddev)\n",
    "\n",
    "    # get the minimum and maximum values of the latent space means\n",
    "    # we will use this as boundaries from which we sample in the latent space\n",
    "    latent_min = np.min(mean_mu - mean_stddev * 2, axis=1)\n",
    "    latent_max = np.max(mean_mu + mean_stddev * 2, axis=1)\n",
    "\n",
    "    # Produce a 20x20 2D grid of evenly spaced values between latent_min ant latent_max\n",
    "    decoded, step_size = sample_on_grid(latent_min, latent_max, model, grid_size=20)\n",
    "    \n",
    "    # visualize the decoded images\n",
    "    # using reshape and np.concatenate and calling imshow only once\n",
    "    # is a lot faster than creating a subplot and calling imshow 400 times.\n",
    "    # It's also necessary to be able to plot the means\n",
    "    block_img = np.concatenate(np.concatenate(decoded, axis=1), axis=1)\n",
    "    f = plt.figure(figsize=(12,12))\n",
    "    plt.imshow(block_img, cmap='Greys')\n",
    "    plt.axis('off');\n",
    "\n",
    "    # visualize the mean mu and the mean standard deviation of each class\n",
    "    # scale the mean accordingly, as the plot's axes represent pixels\n",
    "    scale = (28 / step_size)\n",
    "\n",
    "    mean_mu_scaled = 28/2 + scale.reshape(2, 1) * (mean_mu - latent_min.reshape(2,1))\n",
    "    # we scale the stddev by 2 so the arrows are extended a bit more\n",
    "    mean_stddev_scaled = mean_stddev * scale.reshape(2, 1) * 2\n",
    "    # plot the std deviation\n",
    "    plt.errorbar(mean_mu_scaled[0], mean_mu_scaled[1],\n",
    "                 yerr=mean_stddev_scaled[1], xerr=mean_stddev_scaled[0],\n",
    "                 linestyle='None')\n",
    "    # plot the means\n",
    "    plt.scatter(mean_mu_scaled[0], mean_mu_scaled[1])\n",
    "\n",
    "    for i, txt in enumerate([str(i) for i in range(0,10)]):\n",
    "        plt.annotate(txt, (mean_mu_scaled[0][i] + 3, mean_mu_scaled[1][i] - 3),\n",
    "                     color='blue',\n",
    "                     bbox={'facecolor':'white', 'alpha':0.9, 'pad':1, 'edgecolor':'none'})\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_space(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the influence of the KL-divergence in the loss\n",
    "\n",
    "Let's now investigate the influence of the KL-divergence by training\n",
    "- one model where we weight the KL-divergence part of the loss by a factor of 30 and\n",
    "- one model where we remove it (weight 0).\n",
    "\n",
    "Visualize the latent space as we did above.\n",
    "\n",
    "**ToDo** What do you observe? How can these results be explained? What is the role of the KL divergence term?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START TODO ########################\n",
    "# model_large_kl = \n",
    "## END TODO ########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(model_large_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START TODO ########################\n",
    "# model_no_kl =\n",
    "## END TODO ########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_space(model_no_kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 7.2\n",
    "\n",
    "* **Major Problems: ** TODO\n",
    "* **Helpful? ** TODO\n",
    "* **Duration (hours): ** TODO\n",
    "* **Other feedback: **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
