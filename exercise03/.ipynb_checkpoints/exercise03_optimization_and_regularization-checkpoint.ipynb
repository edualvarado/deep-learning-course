{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Third Exercise (Chapter 7 & 8)\n",
    "This exercise focuses on optimization and regularization.\n",
    "\n",
    "In the optimization part, we will\n",
    "- implement the Adam optimizer\n",
    "- compare Adam, SGD, SGD with Momentum\n",
    "- implement and analyze some learning rate schedules\n",
    "\n",
    "In the regularization part, we will\n",
    "- implement Dropout\n",
    "- implement L1/L2 loss\n",
    "- analyze the effect of the different regularization methods on the parameter distribution.\n",
    "- think about early stopping and data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can **reuse the code you wrote in the last exercise, or you can use the code we provide (below)**. Just copy the relevant parts into the cell below. Please **note that we extended the `Module` class** with a state to determine whether we're training or evaluating and two functions to toggle this state. We therefore also adapted the training loop function.\n",
    "\n",
    "The first task of this exercise is to implement Adam, you can **skip forward to the exercises by clicking [here](#adam)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base classes: `Parameter` and `Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List, Optional, Tuple, Callable\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter:\n",
    "    \"\"\"A trainable parameter.\n",
    "\n",
    "    This class not only stores th_solution-Copy1e value of the parameter but also tensors/\n",
    "    properties associated with it, such as the gradient of the current backward\n",
    "    pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, name: Optional[str] = None):\n",
    "        self.data = data\n",
    "        self.grad = None\n",
    "        self.name = name\n",
    "        self.state_dict = dict()  # dict to store additional, optional information\n",
    "\n",
    "\n",
    "class Module:\n",
    "    \"\"\"The base class all network modules must inherit from.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Cache of the input of the forward pass.\n",
    "        # We need it during the backward pass in most layers,\n",
    "        #  e.g., to compute the gradient w.r.t to the weights.\n",
    "        self.input_cache = None\n",
    "        self.training = True\n",
    "\n",
    "    def __call__(self, *args) -> np.ndarray:\n",
    "        \"\"\"Alias for forward, convenience function.\"\"\"\n",
    "        return self.forward(*args)\n",
    "\n",
    "    def forward(self, *args) -> np.ndarray:\n",
    "        \"\"\"Compute the forward pass through the module.\n",
    "\n",
    "        Args:\n",
    "           args: The inputs, e.g., the output of the previous layer.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the backward pass through the module.\n",
    "\n",
    "        This method computes the gradients with respect to the trainable\n",
    "        parameters and with respect to the first input.\n",
    "        If the module has trainable parameters, this method needs to update\n",
    "        the respective parameter.grad property.\n",
    "\n",
    "        Args:\n",
    "            grad: The gradient of the following layer.\n",
    "\n",
    "        Returns:\n",
    "            The gradient with respect to the first input argument. In general\n",
    "            it might be useful to return the gradients w.r.t. to all inputs, we\n",
    "            omit this here to keep things simple.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        \"\"\"Return the module parameters.\"\"\"\n",
    "        return []  # default to empty list\n",
    "\n",
    "    def train(self, mode : bool = True) -> 'Module':\n",
    "        \"\"\"Set the module to training mode.\n",
    "\n",
    "        This only affects some Modules, such as Dropout.\n",
    "        \n",
    "        Returns:\n",
    "            self.\n",
    "        \"\"\"\n",
    "        self.training = mode\n",
    "        return self\n",
    "\n",
    "    def eval(self) -> 'Module':\n",
    "        \"\"\"Set the module to evaluation mode.\n",
    "\n",
    "        This only affects some Modules, such as Dropout.\n",
    "\n",
    "        Returns:\n",
    "            self.\n",
    "        \"\"\"\n",
    "        return self.train(False)\n",
    "\n",
    "    def check_gradients(self, input_args: Tuple[np.ndarray]):\n",
    "        \"\"\"Verify the implementation of the gradients.\n",
    "\n",
    "        This includes the gradient with respect to the input as well as the\n",
    "        gradients w.r.t. the parameters if the module contains any.\n",
    "\n",
    "        As the scipy grad check only works on scalar functions, we compute\n",
    "        the sum over the output to obtain a scalar.\n",
    "        \"\"\"\n",
    "        assert isinstance(input_args, tuple), (\n",
    "            \"input_args must be a tuple but is {}\".format(type(input_args)))\n",
    "        TOLERANCE = 1e-6\n",
    "        self.check_gradients_wrt_input(input_args, TOLERANCE)\n",
    "        self.check_gradients_wrt_params(input_args, TOLERANCE)\n",
    "\n",
    "    def _zero_grad(self):\n",
    "        \"\"\"(Re-) intialize the param's grads to 0. Helper for grad checking.\"\"\"\n",
    "        for p in self.parameters():\n",
    "            p.grad = np.zeros_like(p.data)\n",
    "\n",
    "    def check_gradients_wrt_input(self, input_args: Tuple[np.ndarray],\n",
    "                                  tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. input.\"\"\"\n",
    "\n",
    "        def output_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.forward for scipy.optimize.check_grad.\"\"\"\n",
    "            # we only compute the gradient w.r.t. to the first input arg.\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            return np.sum(self.forward(*args))\n",
    "\n",
    "        def grad_given_input(x: np.ndarray):\n",
    "            \"\"\"Wrap self.backward for scipy.optimize.check_grad.\"\"\"\n",
    "            self._zero_grad()\n",
    "            # run self.forward to store the new input\n",
    "            args = (x.reshape(input_args[0].shape),) + input_args[1:]\n",
    "            out = self.forward(*args)\n",
    "            # compute the gradient w.r.t. to the input\n",
    "            return np.ravel(self.backward(np.ones_like(out)))\n",
    "\n",
    "        error = scipy.optimize.check_grad(\n",
    "            output_given_input, grad_given_input, np.ravel(input_args[0]))\n",
    "        num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "        if np.squeeze(error) / num_outputs > tolerance:\n",
    "            raise RuntimeError(\"Check of gradient w.r.t. to input for {} failed.\"\n",
    "                               \"Error {:.4E} > {:.4E}.\"\n",
    "                               .format(self, np.squeeze(error), tolerance))\n",
    "\n",
    "    def check_gradients_wrt_params(self, input_args: Tuple[np.ndarray],\n",
    "                                   tolerance: float):\n",
    "        \"\"\"Verify the implementation of the module's gradient w.r.t. params.\"\"\"\n",
    "        for param in self.parameters():\n",
    "            def output_given_params(new_param: np.ndarray):\n",
    "                \"\"\"Wrap self.forward, change the parameters to new_param.\"\"\"\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                return np.sum(self.forward(*input_args))\n",
    "\n",
    "            def grad_given_params(new_param: np.ndarray):\n",
    "                self._zero_grad()\n",
    "                param.data = new_param.reshape(param.data.shape)\n",
    "                out = self.forward(*input_args)\n",
    "                # compute the gradient w.r.t. to param\n",
    "                self.backward(np.ones_like(out))\n",
    "                return np.ravel(param.grad)\n",
    "            # flatten the param as scipy can only handle 1D params\n",
    "            param_init = np.ravel(np.copy(param.data))\n",
    "            error = scipy.optimize.check_grad(output_given_params,\n",
    "                                              grad_given_params,\n",
    "                                              param_init)\n",
    "            num_outputs = np.prod(self.forward(*input_args).shape)\n",
    "            if np.squeeze(error) / num_outputs > tolerance:\n",
    "                raise RuntimeError(\"Check of gradient w.r.t. to param '{}' for\"\n",
    "                                   \"{} failed. Error {:.4E} > {:.4E}.\"\n",
    "                                   .format(param.name, self, error, tolerance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions: `Relu` and `Softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        # Shift input for numerical stability.\n",
    "        reduction_axes = tuple(range(1, len(z.shape)))\n",
    "        shift_z = z - np.max(z, axis=reduction_axes, keepdims=True)\n",
    "        exps = np.exp(shift_z)\n",
    "        h = exps / np.sum(exps, axis=reduction_axes, keepdims=True)\n",
    "        return h\n",
    "\n",
    "    def backward(self, grad) -> np.ndarray:\n",
    "        error_msg = (\"Softmax doesn't need to implement a gradient here, as it's\"\n",
    "                     \"only needed in CrossEntropyLoss, where we can simplify\"\n",
    "                     \"the gradient for the combined expression.\")\n",
    "        raise NotImplementedError(error_msg)\n",
    "\n",
    "\n",
    "class Relu(Module):\n",
    "\n",
    "    def forward(self, z: np.ndarray) -> np.ndarray:\n",
    "        self.input_cache = z\n",
    "        return np.maximum(0, z)\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        z = self.input_cache\n",
    "        return grad.reshape(z.shape) * np.where(z > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        super().__init__()\n",
    "        self.W = Parameter(np.random.randn(out_features, in_features) * 0.01,\n",
    "                           name=\"W\")\n",
    "        self.b = Parameter(np.ones((out_features, 1)) * 0.01, name=\"b\")\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        assert len(x.shape) == 3, (\"x.shape should be (batch_size, input_size, 1)\"\n",
    "                                   \" but is {}.\".format(x.shape))\n",
    "        self.input_cache = x\n",
    "        z = self.W.data @ x + self.b.data\n",
    "        return z\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        x = self.input_cache\n",
    "        # remember that we have a batch dimension when transposing, i.e.,\n",
    "        # we need to use np.transpose instead of array.T\n",
    "        self.W.grad += np.sum(grad @ np.transpose(x, [0, 2, 1]), axis=0)\n",
    "        self.b.grad += np.sum(grad, axis=0)\n",
    "        return self.W.data.T @ grad\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        return self.W, self.b\n",
    "\n",
    "\n",
    "class Sequential(Module):\n",
    "    \"\"\"A sequential container to stack modules.\n",
    "\n",
    "    Modules will be added to it in the order they are passed to the\n",
    "    constructor.\n",
    "\n",
    "    Example network with one hidden layer:\n",
    "    model = Sequential(\n",
    "                  Linear(5,10),\n",
    "                  ReLU(),\n",
    "                  Linear(10,10),\n",
    "                )\n",
    "    \"\"\"\n",
    "    def __init__(self, *args: List[Module]):\n",
    "        super().__init__()\n",
    "        self.modules = args\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        for module in self.modules:\n",
    "            x = module(x)  # equivalent to module.forward(x)\n",
    "        return x\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        for module in reversed(self.modules):\n",
    "            grad = module.backward(grad)\n",
    "        return grad\n",
    "\n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        # iterate over modules and retrieve their parameters, iterate over\n",
    "        # parameters to flatten the list\n",
    "        return [param for module in self.modules\n",
    "                for param in module.parameters()]\n",
    "    \n",
    "    def train(self, mode: bool = True) -> 'Sequential':\n",
    "        \"\"\"Set the train mode of the Sequential module and it's sub-modules.\n",
    "        \n",
    "        This only affects some modules, e.g., Dropout.\n",
    "        \n",
    "        Returns:\n",
    "            self.\n",
    "        \"\"\"\n",
    "        for module in self.modules:\n",
    "            module.train(mode)\n",
    "        return self\n",
    "\n",
    "\n",
    "def one_hot_encoding(y: np.ndarray, num_classes: int) -> np.ndarray:\n",
    "    \"\"\"Convert integer labels to one hot encoding.\n",
    "\n",
    "    Example: y=[1, 2] --> [[0, 1, 0], [0, 0, 2]]\n",
    "    \"\"\"\n",
    "    encoded = np.zeros(y.shape + (num_classes,))\n",
    "    encoded[np.arange(len(y)), y] = 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"The base class for optimizers.\n",
    "\n",
    "    All optimizers must implement a step() method that updates the parameters.\n",
    "    The general optimization loop then looks like this:\n",
    "\n",
    "    for inputs, targets in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    `zero_grad` initializes the gradients of the parameters to zero. This\n",
    "    allows to accumulate gradients (instead of replacing it) during\n",
    "    backpropagation, which is e.g. useful for skip connections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[Parameter]):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            params: The parameters to be optimized.\n",
    "        \"\"\"\n",
    "        self._params = params\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"Update the parameters.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self) -> None:\n",
    "        \"\"\"Clear the gradients of all optimized parameters.\"\"\"\n",
    "        for param in self._params:\n",
    "            assert isinstance(param, Parameter)\n",
    "            param.grad = np.zeros_like(param.data)\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Stochastic Gradient Descent (SGD) optimizer with optional Momentum.\"\"\"\n",
    "\n",
    "    def __init__(self, params: Iterable[Parameter], lr: float,\n",
    "                 momentum: Optional[float] = None):\n",
    "        super().__init__(params)\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        if momentum:\n",
    "            for param in self._params:\n",
    "                param.state_dict[\"momentum\"] = np.zeros_like(param.data)\n",
    "\n",
    "    def step(self):\n",
    "        for p in self._params:\n",
    "            if self.momentum:\n",
    "                # update the momentum\n",
    "                p.state_dict[\"momentum\"] *= self.momentum\n",
    "                p.state_dict[\"momentum\"] -= self.lr * p.grad\n",
    "                # update the parameter\n",
    "                p.data += p.state_dict[\"momentum\"]\n",
    "            else:\n",
    "                p.data -= self.lr * p.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    \"\"\"Compute the cross entropy.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "    def forward(self, a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute the cross entropy, mean over batch size.\"\"\"\n",
    "        a = self.softmax(a)\n",
    "        self.input_cache = a, y\n",
    "        # compute the mean over the batch\n",
    "        return -np.sum(np.log(a[y == 1])) / len(a)\n",
    "\n",
    "    def backward(self, _=None) -> np.ndarray:\n",
    "        # we introduce the argument _ here, to have a unified interface with\n",
    "        # other Module objects. This simplifies code for gradient checking. \n",
    "        # We don't need this arg.\n",
    "        a, y = self.input_cache\n",
    "        grad = (a - y) / len(a)\n",
    "\n",
    "        # We have to recreate the batch dimension\n",
    "        grad = np.expand_dims(grad, -1)\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, predictions, y_is_onehot: bool = False) -> float:\n",
    "    y_predicted = np.argmax(predictions, axis=-1)\n",
    "    y = np.argmax(y, axis=-1)\n",
    "    return np.sum(np.equal(y_predicted, y)) / len(y)\n",
    "\n",
    "\n",
    "def evaluate(data, labels, model, loss_fn, batch_size):\n",
    "    predictions = []\n",
    "    eval_cost = 0.\n",
    "    data_batched = minibatched(data, batch_size)\n",
    "    labels_batched = minibatched(labels, batch_size)\n",
    "\n",
    "    for x, y in zip(data_batched, labels_batched):\n",
    "        # note that when using cross entropy loss, the softmax is included in the\n",
    "        # loss and we'd need to apply it manually here to obtain the output as probabilities.\n",
    "        # However, softmax only rescales the outputs and doesn't change the argmax,\n",
    "        # so we'll skip this here, as we're only interested in the class prediction.\n",
    "        h_1 = np.squeeze(model(x))\n",
    "        predictions.append(h_1)\n",
    "        eval_cost += loss_fn(h_1, y)\n",
    "    predictions = np.array(predictions).reshape(-1, 10)\n",
    "    eval_accuracy = accuracy(y_val, predictions, False)\n",
    "    return eval_accuracy, eval_cost\n",
    "\n",
    "\n",
    "def train(model, loss_fn, optimizer, x_train, y_train, x_val, y_val,\n",
    "          num_epochs, batch_size, scheduler=None):\n",
    "    train_costs, train_accuracies = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    eval_costs, eval_accuracies = np.zeros(num_epochs), np.zeros(num_epochs)\n",
    "    ix = np.arange(len(x_train))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {} / {}:\".format(epoch + 1, num_epochs))\n",
    "        training_predictions = []\n",
    "        \n",
    "        np.random.shuffle(ix)\n",
    "        x_train_batched = minibatched(x_train[ix], batch_size)\n",
    "        y_train_batched = minibatched(y_train[ix], batch_size)\n",
    "        \n",
    "        # train for one epoch\n",
    "        model.train()\n",
    "        for x_batch, y_batch in zip(x_train_batched, y_train_batched):\n",
    "            optimizer.zero_grad()\n",
    "            y_batch_predicted = model(x_batch)\n",
    "            h_1 = np.squeeze(y_batch_predicted)\n",
    "            training_predictions.append(h_1)\n",
    "            loss = loss_fn(h_1, y_batch)\n",
    "            grad = loss_fn.backward()\n",
    "            model.backward(grad)\n",
    "            optimizer.step()\n",
    "            train_costs[epoch] += loss\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        model.eval()\n",
    "       \n",
    "        training_predictions = np.array(training_predictions).reshape(-1, 10)\n",
    "        train_accuracies[epoch] = accuracy(y_train[ix], training_predictions, False)\n",
    "        print(\"  Training Accuracy: {:.4f}\".format(train_accuracies[epoch]))\n",
    "        print(\"  Training Cost: {:.4f}\".format(train_costs[epoch]))\n",
    "        eval_accuracies[epoch], eval_costs[epoch] = evaluate(x_val, y_val, model, loss_fn, batch_size)\n",
    "        print(\"  Eval Accuracy: {:.4f}\".format(eval_accuracies[epoch]))\n",
    "    return train_costs, train_accuracies, eval_costs, eval_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_data():\n",
    "    \"\"\"Loads the data, returns training_data, validation_data, test_data.\"\"\"\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as f:\n",
    "        return pickle.load(f, encoding='latin1')\n",
    "\n",
    "\n",
    "def minibatched(data: np.ndarray, batch_size: int) -> List[np.ndarray]:\n",
    "    assert len(data) % batch_size == 0, (\"Data length {} is not multiple of batch size {}\"\n",
    "                                         .format(len(data), batch_size))\n",
    "    return data.reshape(-1, batch_size, *data.shape[1:])\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_val, y_val), (x_test, y_test) = load_mnist_data()\n",
    "x_train = np.expand_dims(x_train, axis=-1)\n",
    "x_val = np.expand_dims(x_val, axis=-1)\n",
    "x_test = np.expand_dims(x_test, axis=-1)\n",
    "\n",
    "num_classes = 10\n",
    "y_train = one_hot_encoding(y_train, num_classes)\n",
    "y_val = one_hot_encoding(y_val, num_classes)\n",
    "y_test = one_hot_encoding(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='adam'></a>\n",
    "### Adam\n",
    "**Implement the step function** of the adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=0.001, betas=(0.9, 0.999), eps=1e-08):\n",
    "        super().__init__(params)\n",
    "        # we stick to the pytorch API, the variable names corresponding\n",
    "        # to the DL book are given in the comments\n",
    "        self.lr = lr  # lr is called epsilon in the DL book\n",
    "        self.betas = betas  # betas are called rho in the DL book\n",
    "        self.eps = eps  # eps is called delta in the DL book\n",
    "        self.t = 0\n",
    "        for param in self._params:\n",
    "            # first order moment variables, called m in the paper\n",
    "            param.state_dict[\"s\"] = np.zeros_like(param.data)\n",
    "            # second order moment variables, called v in the paper\n",
    "            param.state_dict[\"r\"] = np.zeros_like(param.data)\n",
    "\n",
    "    def step(self) -> None:\n",
    "        \"\"\"Update the parameters and decaying averages of past gradients.\"\"\"\n",
    "        # START TODO ################\n",
    "\n",
    "        # END TODO###################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare: Adam vs SGD vs SGD with Momentum\n",
    "\n",
    "**Train three models** (30 hidden units, relu) for 10 epochs, one with *Adam*, one with *SGD* and one with *SGD with momentum*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_units = 30\n",
    "batch_size = 50\n",
    "num_epochs = 10\n",
    "sgd_learning_rate = 0.1\n",
    "momentum = 0.9\n",
    "adam_learning_rate = 0.01\n",
    "\n",
    "# START TODO ################\n",
    "\n",
    "# END TODO###################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create two plots**, one for the training loss curves and one for the training accuracies curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "% matplotlib inline\n",
    "\n",
    "# START TODO ################\n",
    "\n",
    "# END TODO###################D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Schedules\n",
    "\n",
    "Decreasing the learning rate can improve performance of the model.\n",
    "\n",
    "Pytorch implements learning rate schedules as wrappers for the optimizer and requires the user to call scheduler.step() after each epoch. (We've already done this for you in the training loop above)\n",
    "\n",
    "**Implement the learning rate scheduler** `PieceWiseConstantLR` and `CosineAnnealingLR`. You can use the provided `LambdaLR` class for this, which works analogously to the pytorch [LambdaLR](https://pytorch.org/docs/master/optim.html#torch.optim.lr_scheduler.LambdaLR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR:\n",
    "\n",
    "    def __init__(self, optimizer: Optimizer, lr_lambda: Callable[[int], float]):\n",
    "        \"\"\"Sets the learning rate to the initial lr times a given function.\n",
    "\n",
    "        Args:\n",
    "            optimizer: The optimzier to wrap.\n",
    "            lr_lambda: A function that takes the current epoch as an argument\n",
    "                and returns the corresponding learning rate.\n",
    "        \"\"\"\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "        self.last_epoch = 0\n",
    "        self.initial_lr = np.copy(optimizer.lr)\n",
    "        self.lr_lambda = lr_lambda\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"To be called after each epoch. Update optimizer.lr\"\"\"\n",
    "        self.last_epoch += 1\n",
    "        self.optimizer.lr = self.lr_lambda(self.last_epoch)\n",
    "\n",
    "\n",
    "class PiecewiseConstantLR(LambdaLR):\n",
    "\n",
    "    def __init__(self, optimizer: Optimizer, epochs: List[int],\n",
    "                 learning_rates: List[float]):\n",
    "        \"\"\"Set learning rate as piecewise constant steps.\n",
    "\n",
    "        This class inherits from LambdaLR and implements the lambda\n",
    "        function that maps the current epoch to the learning rate\n",
    "        according to epochs.\n",
    "        \n",
    "        optimizer: The optimizer to wrap\n",
    "        \"\"\"\n",
    "        # START TODO ################\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # End TODO ################\n",
    "        \n",
    "        \n",
    "class CosineAnnealingLR(LambdaLR):\n",
    "\n",
    "    def __init__(self, optimizer: Optimizer, T_max: int):\n",
    "        \"\"\"Set learning rate as a cosine decay.\n",
    "\n",
    "        This class inherits from LambdaLR and implements the lambda\n",
    "        function that maps the current epoch to the learning rate\n",
    "        according to epochs.\n",
    "        \n",
    "        optimizer: The optimizer to wrap\n",
    "        T_max:  Maximum number of epochs.\n",
    "        \"\"\"\n",
    "        # START TODO ################\n",
    "        raise NotImplementedError\n",
    "        # End TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify your implementation, **plot the learning rate schedules**, with the number of epochs on the x-axis and the learning rate on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 80\n",
    "\n",
    "optimizer = Adam([])\n",
    "picewise_scheduler = PiecewiseConstantLR(optimizer, [10, 20, 40, 50], [0.1, 0.05, 0.01, 0.001])\n",
    "\n",
    "optimizer = Adam([])\n",
    "cosine_scheduler = CosineAnnealingLR(optimizer, num_epochs)\n",
    "\n",
    "# START TODO ################\n",
    "# plot piecewise lr\n",
    "\n",
    "# plot cosine lr\n",
    "# End TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 3.1\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "In this part of the exercise we will take a look multiple regularizers.\n",
    "\n",
    "### Dropout\n",
    "**Fill in the missing gaps** for the implementation of the dropout regularization (Chapter 7.12 in the DL book).\n",
    "During training, the dropout layer randomly sets the input tensor to 0 with probability p and scales the remaining values accordingly. During evaluation, the dropout layer returns the identity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Module):\n",
    "    \"\"\"Set input elements to zero during training with probability p.\"\"\"\n",
    "\n",
    "    def __init__(self, p : float = 0.5, fix_seed=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            p: Probability of an element to be zeroed. Default: 0.5.\n",
    "            fix_seed: If true, we always use the same seed in the forward pass.\n",
    "                This is only needed for gradient checking and should only be\n",
    "                set True for gradient checking.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        # START TODO ################\n",
    "        # self.scale = TODO\n",
    "        # END TODO ################\n",
    "        self.fix_seed = fix_seed\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply dropout during training.\n",
    "        \n",
    "        Set values to zero with probability p during training\n",
    "        and scale them by 1/(1-p). Returns identidy during\n",
    "        evaluation mode (--> self.training = False).\n",
    "\n",
    "        Note: This layer should work with all kinds of input shapes.\n",
    "        \"\"\" \n",
    "        if not self.training:\n",
    "            return x\n",
    "        if self.fix_seed:  # we need this for gradient checking \n",
    "            np.random.seed(0)\n",
    "        # START TODO ################\n",
    "\n",
    "        # END TODO ################\n",
    "\n",
    "    def backward(self, grad: np.ndarray) -> np.ndarray:\n",
    "        if not self.training:\n",
    "            raise ValueError(\"Model is set to evaluation mode.\")\n",
    "        # START TODO ################\n",
    "\n",
    "        # END TODO ################\n",
    "\n",
    "\n",
    "# Check the gradient implementation\n",
    "x = np.random.rand(1, 1, 4, 4)\n",
    "Dropout(fix_seed=True).check_gradients((x,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1/L2 regularization\n",
    "**Implement $L_1$/$L_2$ regularization**. This is one of the rare cases where we differ from the pytorch API. The reason is that pytorch implements $L_2$ regularization in the optimizer but calles it `weight_decay` (which is actually a different operation if you're not using SGD).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Regularization(Module):\n",
    "\n",
    "    def __init__(self, alpha: float, parameters: List[Parameter]):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.params = parameters\n",
    "    \n",
    "    def forward(self) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "\n",
    "        # END TODO ################\n",
    "\n",
    "    def backward(self, _=None) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "\n",
    "        # END TODO ################\n",
    "        \n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        return self.params\n",
    "\n",
    "        \n",
    "class L2Regularization(Module):\n",
    "\n",
    "    def __init__(self, alpha: float, parameters: List[Parameter]):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.params = parameters\n",
    "    \n",
    "    def forward(self) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "\n",
    "        # END TODO ################\n",
    "\n",
    "    def backward(self, _=None) -> np.ndarray:\n",
    "        # START TODO ################\n",
    "\n",
    "        # END TODO ################\n",
    "        \n",
    "    def parameters(self) -> List[Parameter]:\n",
    "        return self.params\n",
    "\n",
    "\n",
    "# Check the gradient implementation. Here we can only check w.r.t. the parameters.\n",
    "params = [Parameter(np.random.rand(50, 1) * 0.1)]\n",
    "l2 = L2Regularization(0.1, params)\n",
    "l1 = L1Regularization(0.1, params)\n",
    "l2.check_gradients_wrt_params((), 1e-6)\n",
    "l1.check_gradients_wrt_params((), 1e-6)\n",
    "\n",
    "\n",
    "class RegularizedCrossEntropy(Module):\n",
    "    \"\"\"Combines Cross Entropy loss and Regularization loss by summing them.\"\"\"\n",
    "    \n",
    "    def __init__(self, regularization_loss: Module):\n",
    "        self.reg_loss = regularization_loss\n",
    "        self.cross_entropy = CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, a: np.ndarray, y: np.ndarray) -> np.ndarray:\n",
    "        return self.cross_entropy(a, y) + self.reg_loss()\n",
    "        \n",
    "    def backward(self, _=None) -> np.ndarray:\n",
    "        self.reg_loss.backward()  # this updates parameter gradients, no grad w.r.t input\n",
    "        return self.cross_entropy.backward()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now visualize the effect of regularization on the model parameters.\n",
    "We'll do that with **four trained models**:\n",
    "1. Without regularization. It's always a good idea to train your model without any regularization first. Not only to have a baseline but also to check if your model is able to overfit the training data. If it can't overfit, it's likely not powerful enough or there's an error in the implementation. \n",
    "2. With L2 regularization, alpha = 0.0001, on the weight parameters (not on the bias)\n",
    "3. With L1 regularization, alpha = 0.0001, on the weight parameters (not on the bias)\n",
    "4. With Dropout, drop_probability = 0.3 for the input, drop_probability = 0.5 for the hidden layers.\n",
    "\n",
    "*Note* that in this case L1/L2/Dropout lead to worse accuracies. This is due to the fact that we use only 30 linear units to keep training times short, which already imposes strong regularization. In practice, we would select a more powerful model. However, the small model still nicely exhibits the effect of regularization on the distribution of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.05\n",
    "momentum = 0.9\n",
    "linear_units = 30\n",
    "alpha_l1 = 0.0001\n",
    "alpha_l2 = 0.0001\n",
    "\n",
    "def build_model():\n",
    "    return Sequential(Linear(784, linear_units),\n",
    "                      Relu(),\n",
    "                      Linear(linear_units, 10))\n",
    "\n",
    "\n",
    "def build_model_dropout():\n",
    "    return Sequential(Dropout(0.3),\n",
    "                      Linear(784, linear_units),\n",
    "                      Relu(),\n",
    "                      Dropout(0.5),\n",
    "                      Linear(linear_units, 10))\n",
    "\n",
    "models = {}  # dict to store the trained models\n",
    "# let's save a model, which we won't train, to get the initial parameter distribution\n",
    "models[\"before_training\"] = build_model()\n",
    "\n",
    "# no regularization\n",
    "print(\"No regularization.\")\n",
    "models[\"no_reg\"] = build_model()\n",
    "params = [p for p in models[\"no_reg\"].parameters() if \"W\" in p.name]\n",
    "cross_entropy = CrossEntropyLoss()\n",
    "optimizer = SGD(models[\"no_reg\"].parameters(), lr=learning_rate, momentum=momentum)\n",
    "train(models[\"no_reg\"], cross_entropy, optimizer, x_train, y_train,\n",
    "      x_val, y_val, num_epochs=num_epochs, batch_size=batch_size)\n",
    "\n",
    "# START TODO ################\n",
    "\n",
    "\n",
    "# L2 regularization\n",
    "print(\"L2 regularization.\")\n",
    "\n",
    "\n",
    "\n",
    "# L1 regularization\n",
    "print(\"L1 regularization.\")\n",
    "\n",
    "\n",
    "\n",
    "# dropout\n",
    "print(\"Dropout.\")\n",
    "\n",
    "\n",
    "# END TODO ################\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now compare the parameter distribution of the four models by **plotting the histogram of their weight values from -1 to 1 with 100 bins**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# START TODO ################\n",
    "\n",
    "# End TODO ################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "Data augmentation increases model generalization by increasing the training set with fake data.\n",
    "\n",
    "**Question:** State *five effective operations* to generate fake data on the *MNIST dataset*. State *one operation* which doesn't make sense and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Another very popular technique in deep learning is *early stopping* (deeplearning book, section 7.8). \n",
    "\n",
    "![Figure 7.3 from the DeepLearningBook](learning-curve-dl-fig-7-3.png \"TEST\")\n",
    "\n",
    "**Questions:**\n",
    "How do the given loss curves (deep learning book, figure 7.3) relate to *early stopping*?\n",
    "Why is it a regularization technique? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Feedback on Exercise 3.2\n",
    "\n",
    "TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
